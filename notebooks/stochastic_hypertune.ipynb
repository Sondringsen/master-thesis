{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for hyperparameter-tuning of Stochastic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "# import some data generating model\n",
    "from data_processing.post_processing import post_processing\n",
    "from data_processing.config import post_processing_config\n",
    "from models.DeepHedger import hedging\n",
    "from config import data_generation_config\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train and val data\n",
    "\n",
    "NOTE: \n",
    "- If you need preprocessing of the train data, you can do it here, but it is cleaner if the model class does it so that the input to the data generating model is just the raw data.\n",
    "- The preprocessing of the train does not need to match the preprocessing of val since the postprocessing of the generated data matches the val data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../data/raw/spy_daily_closing_prices_train.csv\", index_col=0)\n",
    "\n",
    "val_data = pd.read_csv(\"../data/raw/spy_daily_closing_prices_val.csv\", index_col=0)\n",
    "\n",
    "val_data = val_data[\"Close\"]\n",
    "val_data.plot(use_index=True)\n",
    "val_data = np.array([val_data.values[i:i+30] for i in range(len(val_data)-30 + 1)])\n",
    "val_data = pd.DataFrame(val_data)\n",
    "val_data = val_data.div(val_data.iloc[:, 0], axis=0)\n",
    "\n",
    "test_data = pd.read_csv(\"../data/raw/spy_daily_closing_prices_test.csv\", index_col=0)\n",
    "test_data = test_data[\"Close\"]\n",
    "test_data = np.array([test_data.values[i:i+30] for i in range(len(test_data)-30 + 1)])\n",
    "test_data = pd.DataFrame(test_data)\n",
    "test_data = test_data.div(test_data.iloc[:, 0], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb login\n",
    "\n",
    "NOTE:\n",
    "- You might want to change this as this is my api key. A bit silly that I push the API-key to a public repo but but..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"52ea61320bbc9ee2b773e909700366e65977cd0f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the sweep config.\n",
    "\n",
    "NOTE:\n",
    "- Here is an example, just change to kappa and theta and whatever for the stochastic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'noise_scale': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.01,\n",
    "            'max': 0.15\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.001,\n",
    "            'max': 0.1\n",
    "        },\n",
    "        'hidden_size': {\n",
    "            'values': [4, 8,16, 32]\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the sweep\n",
    "\n",
    "NOTE:\n",
    "- Set a project name. I have called mine teacher_forcing_hyperopt for instance. So you can call it heston_hyperopt\n",
    "- When you run this a sweep id will be printed. If you want to cancel and resume a sweep you can do that just comment out the wandb.sweep line and uncomment the sweep_id line and set sweep_id equal to the sweep_id that you generated the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
    "# sweep_id = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the functions here\n",
    "\n",
    "NOTE:\n",
    "- You might need to make some changes here depending on how you train the model and generate the data.\n",
    "- Important thing: model.synth_data has to be a (MxN) dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(config):\n",
    "    hyperparameters = {\n",
    "            \"noise_scale\": config.noise_scale,\n",
    "            \"learning_rate\": config.learning_rate,\n",
    "            \"hidden_size\": config.hidden_size\n",
    "        }\n",
    "    \n",
    "    # model = TeacherForcing(train_data=train_data, N=data_generation_config[\"N\"], M=data_generation_config[\"M\"], load_params=False, config=hyperparameters)\n",
    "    model.generate_data()\n",
    "\n",
    "    df_post_processed = post_processing(model.synth_data, **post_processing_config)\n",
    "\n",
    "    print('Hedging')\n",
    "    val_loss = hedging(df_post_processed, val_data)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "def main():\n",
    "    wandb.init()\n",
    "    val_loss = objective(wandb.config)\n",
    "    wandb.log({\"val_loss\": val_loss,  \"loss\": val_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, function=main, count=10, project=project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "NOTE:\n",
    "- Set the model_name to the name of the model, i.e. quant_gan\n",
    "- Of course, also, set the optimal hyperparameters\n",
    "- Remember to create the folders data/processed and data/performance from root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"\"\n",
    "optimal_hyperparameters = {\n",
    "    \"clip_value\": 0.04846214746770365,\n",
    "    \"lr\": 0.0011239390884900244,\n",
    "    \"num_epochs\": 15,\n",
    "    \"nz\": 3,\n",
    "    \"batch_size\": 64,\n",
    "    \"seq_len\": 30,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    # model = TeacherForcing(train_data=train_data, N=data_generation_config[\"N\"], M=data_generation_config[\"M\"], load_params=False, config=hyperparameters)\n",
    "    model.fit_to_data()\n",
    "\n",
    "    model.generate_data()\n",
    "    df = 1*np.exp(model.synth_data)\n",
    "\n",
    "    df_post_processed = post_processing(df, **post_processing_config)\n",
    "\n",
    "    val_loss, test_loss = hedging(df_post_processed, val_data, test_data)\n",
    "    print(f\"Validation loss: {val_loss}, Test loss: {test_loss}\")\n",
    "    test_losses.append(test_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "print(f\"Average validation loss: {np.mean(val_losses)}, Average test loss: {np.mean(test_losses)}\")\n",
    "df_post_processed.to_csv(f\"../data/processed/{model_name}_synth_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving mean and std of losses in latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LaTeX table with results\n",
    "val_mean = np.mean(val_losses)\n",
    "val_std = np.std(val_losses)\n",
    "test_mean = np.mean(test_losses)\n",
    "test_std = np.std(test_losses)\n",
    "\n",
    "latex_table = f\"\"\"\n",
    "\\\\begin{{table}}[h]\n",
    "\\\\centering\n",
    "\\\\begin{{tabular}}{{lc}}\n",
    "\\\\hline\n",
    "Metric & Value \\\\\\\\\n",
    "\\\\hline\n",
    "Validation Loss & {val_mean:.6f} $\\\\pm$ {val_std:.6f} \\\\\\\\\n",
    "Test Loss & {test_mean:.6f} $\\\\pm$ {test_std:.6f} \\\\\\\\\n",
    "\\\\hline\n",
    "\\\\end{{tabular}}\n",
    "\\\\caption{{Validation and Test Loss Statistics}}\n",
    "\\\\label{{tab:loss_stats}}\n",
    "\\\\end{{table}}\n",
    "\"\"\"\n",
    "\n",
    "with open(f'..data/performance/{model_name}.tex', 'w') as f:\n",
    "    f.write(latex_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
